{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi GPU\n",
    "\n",
    "Keras 2.0.9 makes it really easy to use multiple GPUs for Data-parallel training. Let's see how it's done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We use the newly added Fashion Mnist dataset from Zalando Research\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "\n",
    "    Label\tDescription\n",
    "     0\t     T-shirt/top\n",
    "     1\t     Trouser\n",
    "     2\t     Pullover\n",
    "     3\t     Dress\n",
    "     4\t     Coat\n",
    "     5\t     Sandal\n",
    "     6\t     Shirt\n",
    "     7\t     Sneaker\n",
    "     8\t     Bag\n",
    "     9\t     Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = np.expand_dims(X_train.astype(\"float\"), -1)\n",
    "X_test = np.expand_dims(X_test.astype(\"float\"), -1)\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "for i in range(1, 10):\n",
    "    plt.subplot(1, 10, i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mean subtraction to the data\n",
    "mean = np.mean(X_train, axis=0)\n",
    "X_train -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "to augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model(input_shape=(28, 28, 1),\n",
    "                      n_classes=10,\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='glorot_normal'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3),\n",
    "                     padding='same',\n",
    "                     input_shape=input_shape,\n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     activation=activation))\n",
    "    model.add(Conv2D(32, (3, 3),\n",
    "                     activation=activation, \n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3),\n",
    "                     padding='same',\n",
    "                     activation=activation,\n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(Conv2D(64, (3, 3),\n",
    "                     activation=activation,\n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,\n",
    "                    activation=activation,\n",
    "                    kernel_initializer=kernel_initializer))\n",
    "    model.add(Dense(n_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer=kernel_initializer))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.training_utils import multi_gpu_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if more than one GPU is present on the machine we need to create a copy of the model on each GPU and sync them on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpus <= 1:\n",
    "    model = create_conv_model()\n",
    "else:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        model = create_conv_model()\n",
    "    model = multi_gpu_model(model, gpus=gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We will use a `LearningRateScheduler` callback to adjust the learning rate of SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "initial_lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_decay(epoch):\n",
    "    max_epochs = n_epochs\n",
    "    lr = initial_lr\n",
    "    power = 1.0\n",
    "    \n",
    "    alpha = lr * (1 - (epoch / float(max_epochs))) ** power\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "epochs = []\n",
    "lrs = []\n",
    "for i in range(n_epochs):\n",
    "    epochs.append(i)\n",
    "    lrs.append(poly_decay(i))\n",
    "\n",
    "plt.plot(epochs, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=initial_lr, momentum=0.9)\n",
    "callbacks = [LearningRateScheduler(poly_decay)]\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    datagen.flow(X_train, Y_train, batch_size=batch_size * gpus),\n",
    "    validation_data=(X_test, Y_test),\n",
    "    steps_per_epoch=len(X_train) // (batch_size * gpus),\n",
    "    epochs=n_epochs,\n",
    "    callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "your machine has 4 GPUs.\n",
    "- compare the training time with 1 GPU VS 2 GPUs VS 4 GPUs. Is the training time the same? Is is larger or smaller?\n",
    "- try to max out the gpu memory by increasing the batch size. Can you do it?\n",
    "- is the model overfitting? experiment with the model architechture. Try to reduce overfitting by:\n",
    "    - adding more layers\n",
    "    - changing the filter size\n",
    "    - adding more dropout\n",
    "    - adding regularization\n",
    "    - adding batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 CATALIT LLC.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
